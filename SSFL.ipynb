{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"6\"\n",
    "import torch\n",
    "from torch import nn, autograd\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import random\n",
    "from sklearn import metrics\n",
    "import copy\n",
    "import argparse\n",
    "import time\n",
    "import tqdm\n",
    "from collections import Counter\n",
    "from utils import data_iid, data_noniid, GRU, test_img,pesudo_label\n",
    "from utils import  global_train, FedAvg\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def args_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # federated arguments\n",
    "    parser.add_argument('--epochs', type=int, default=100, help=\"rounds of training\")\n",
    "    parser.add_argument('--num_users', type=int, default=20, help=\"number of users: K\")\n",
    "    parser.add_argument('--frac', type=float, default=0.5, help=\"the fraction of clients: C\")\n",
    "    parser.add_argument('--local_ep', type=int, default=5, help=\"the number of local epochs: E\")\n",
    "    parser.add_argument('--local_bs', type=int, default=30, help=\"local batch size: B\")\n",
    "    parser.add_argument('--bs', type=int, default=256, help=\"test batch size\")\n",
    "    parser.add_argument('--lr', type=float, default=0.0005, help=\"learning rate\")\n",
    "    parser.add_argument('--local_label', type=int, default=2, help=\"each client's label\")\n",
    "    parser.add_argument('--glob_ep', type=int, default=100, help=\"number of global epochs\")\n",
    "    parser.add_argument('--split', type=str, default='user', help=\"train-test split type, user or sample\")\n",
    "\n",
    "    # other arguments\n",
    "    parser.add_argument('--iid', action='store_true', help='whether i.i.d or not')\n",
    "    parser.add_argument('--num_classes', type=int, default=5, help=\"number of classes\")\n",
    "    parser.add_argument('--gpu', type=int, default=0, help=\"GPU ID, -1 for CPU\")\n",
    "    parser.add_argument('--stopping_rounds', type=int, default=10, help='rounds of early stopping')\n",
    "    parser.add_argument('--verbose', action='store_true', help='verbose print')\n",
    "    parser.add_argument('--seed', type=int, default=1, help='random seed (default: 1)')\n",
    "    args = parser.parse_known_args()[0]\n",
    "    return args\n",
    "args = args_parser()\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "args.iid=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "filename = 'ssfl_data.pickle'\n",
    "with open(filename, 'rb') as f:\n",
    "    X,Y= pickle.load(f)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y,test_size=0.15, random_state=3)\n",
    "# x_train, x_unlabel, y_train, y_unlabel = train_test_split(x_train, y_train,test_size=0.5749, random_state=3)\n",
    "x_train, x_unlabel, y_train, y_unlabel = train_test_split(x_train, y_train,test_size=0.28746, random_state=3)\n",
    "\n",
    "# x_train, _, y_train, _ = train_test_split(x_train, y_train,test_size=0.5, random_state=3)\n",
    "\n",
    "#0.4312   label 7914  unlabel 6000   test 2456 labeled 0.4834%\n",
    "#0.5749   label 5914  unlabel 8000   test 2456 labeled 0.425%\n",
    "#0.8624   label 1914  unlabel 12000   test 2456 labeled 0.137%\n",
    "assert len(x_train)==len(y_train)\n",
    "assert len(x_unlabel)==len(y_unlabel)\n",
    "\n",
    "x_new=np.zeros_like(x_train)\n",
    "y_new=np.zeros_like(y_train)\n",
    "for i in range(len(x_train)):\n",
    "    x_new[i]=np.fliplr(x_train[i])\n",
    "    y_new[i]=y_train[i]\n",
    "x_train=np.append(x_train,x_new,axis=0)\n",
    "y_train=np.append(y_train,y_new,axis=0)\n",
    "\n",
    "\n",
    "x_train = torch.from_numpy(x_train).float()\n",
    "y_train = torch.from_numpy(y_train)\n",
    "\n",
    "x_test = torch.from_numpy(x_test).float()\n",
    "y_test = torch.from_numpy(y_test)\n",
    "\n",
    "x_unlabel = torch.from_numpy(x_unlabel).float()\n",
    "y_unlabel = torch.from_numpy(y_unlabel)\n",
    "\n",
    "traindata =TensorDataset(x_train.clone().detach(),y_train.clone().detach().long())\n",
    "testdata = TensorDataset(x_test.clone().detach(),y_test.clone().detach().long())\n",
    "\n",
    "train_loader = DataLoader(traindata,batch_size = args.bs,shuffle=False)\n",
    "test_loader = DataLoader(testdata,batch_size=args.bs,shuffle=True)\n",
    "\n",
    "\n",
    "dataset_unlabel=(x_unlabel,y_unlabel)\n",
    "dataset_test=(x_test,y_test)\n",
    "dataset_server=(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     16
    ]
   },
   "outputs": [],
   "source": [
    "def data_iid(dataset, num_users):\n",
    "    num_shards, num_imgs = 40, 100\n",
    "    idx_shard = [i for i in range(num_shards)]\n",
    "    dict_users = {i: np.array([], dtype='int64') for i in range(num_users)}\n",
    "    idxs = np.arange(len(dataset[0]))\n",
    "    labels =dataset[1].int().numpy()\n",
    "    idxs_labels = np.vstack((idxs, labels))\n",
    "    idxs = idxs_labels[0,:]\n",
    "\n",
    "    for i in range(num_users):\n",
    "        rand_set = set(np.random.choice(idx_shard, 2, replace=False))\n",
    "        idx_shard = list(set(idx_shard) - rand_set)\n",
    "        for rand in rand_set:\n",
    "            dict_users[i] = np.concatenate((dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]), axis=0)\n",
    "    return dict_users\n",
    "\n",
    "def data_noniid(dataset, num_users,args):\n",
    "    num_shards, num_imgs = 40, 100\n",
    "    idx_shard = [i for i in range(num_shards)]\n",
    "    dict_users = {i: np.array([], dtype='int64') for i in range(num_users)}\n",
    "    idxs = np.arange(len(dataset[0]))\n",
    "    labels =dataset[1].int().numpy()\n",
    "    idxs_labels = np.vstack((idxs, labels))\n",
    "    idxs_labels = idxs_labels[:,idxs_labels[1,:].argsort()]\n",
    "    idxs = idxs_labels[0,:]\n",
    "\n",
    "    \n",
    "    for i in range(num_users):\n",
    "        rand_set=[]\n",
    "        label_set=[]\n",
    "        count=0\n",
    "        while len(rand_set)<args.local_label:\n",
    "            rs=random.choice(idx_shard)\n",
    "            label=int(dataset[1][idxs[rs*num_imgs]])\n",
    "            count+=1\n",
    "            if label not in set(label_set) or count>10:\n",
    "                label_set.append(label)\n",
    "                rand_set.append(rs)\n",
    "                idx_shard=list(set(idx_shard) - set(rand_set))\n",
    "\n",
    "        for rand in rand_set:\n",
    "            dict_users[i] = np.concatenate((dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]), axis=0)\n",
    "    return dict_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    100\n",
      "0    100\n",
      "dtype: int64\n",
      "4    100\n",
      "1    100\n",
      "dtype: int64\n",
      "2    100\n",
      "0    100\n",
      "dtype: int64\n",
      "4    100\n",
      "0    100\n",
      "dtype: int64\n",
      "3    100\n",
      "1    100\n",
      "dtype: int64\n",
      "3    100\n",
      "0     94\n",
      "1      6\n",
      "dtype: int64\n",
      "3    100\n",
      "1     77\n",
      "2     23\n",
      "dtype: int64\n",
      "1    100\n",
      "0    100\n",
      "dtype: int64\n",
      "4    100\n",
      "1    100\n",
      "dtype: int64\n",
      "2    100\n",
      "1    100\n",
      "dtype: int64\n",
      "4    100\n",
      "1    100\n",
      "dtype: int64\n",
      "4    100\n",
      "0    100\n",
      "dtype: int64\n",
      "3    100\n",
      "0    100\n",
      "dtype: int64\n",
      "4    100\n",
      "2    100\n",
      "dtype: int64\n",
      "2    100\n",
      "0    100\n",
      "dtype: int64\n",
      "3    100\n",
      "2    100\n",
      "dtype: int64\n",
      "3    100\n",
      "2    100\n",
      "dtype: int64\n",
      "4    100\n",
      "3    100\n",
      "dtype: int64\n",
      "4    100\n",
      "2     68\n",
      "3     32\n",
      "dtype: int64\n",
      "1    100\n",
      "3     78\n",
      "4     22\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "args.iid=False\n",
    "if args.iid:\n",
    "    dict_users = data_iid(dataset_unlabel, args.num_users)\n",
    "else:\n",
    "    dict_users = data_noniid(dataset_unlabel, args.num_users,args)\n",
    "\n",
    "for i in range(args.num_users):\n",
    "    print(pd.Series.value_counts(np.array(y_unlabel[list(dict_users[i])])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #build model\n",
    "net_glob = GRU().to(args.device)   \n",
    "net_glob.train()\n",
    "args.lr=0.0005\n",
    "# for epoch in range(1, 80 + 1):\n",
    "#     start=time.time()\n",
    "#     global_train(args, net_glob, epoch, dataset=dataset_server)\n",
    "#     print(\"time {:.2f} sec:\".format(time.time()-start))\n",
    "#     acc_global, loss_gloabl = test_img(net_glob, dataset_test, args)\n",
    "#     print(\"Epoch: {} Testing accuracy in global: {:.2f}\".format(epoch, acc_global))\n",
    "# args.lr=0.0001\n",
    "# for epoch in range(1, 50 + 1):\n",
    "#     start=time.time()\n",
    "#     global_train(args, net_glob, epoch, dataset=dataset_server)\n",
    "#     print(\"time {:.2f} sec:\".format(time.time()-start))\n",
    "#     acc_global, loss_gloabl = test_img(net_glob, dataset_test, args)\n",
    "#     print(\"Epoch: {} Testing accuracy in global: {:.2f}\".format(epoch, acc_global))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     31
    ]
   },
   "outputs": [],
   "source": [
    "# def pesudo_label(args, net_g=None, dataset=None, idxs=None, tao=0.99):\n",
    "#     net_glob.eval()\n",
    "#     idxs = list(idxs)\n",
    "\n",
    "#     image = dataset_unlabel[0][idxs]\n",
    "#     label = dataset_unlabel[1][idxs]\n",
    "#     image = image.numpy()\n",
    "#     label = label.numpy()\n",
    "#     x_pusedo = []\n",
    "#     y_pusedo = []\n",
    "#     y_true=[]\n",
    "#     for i in range(len(image)):\n",
    "#         x_temp = image[i][np.newaxis, :, :]\n",
    "#         x_temp = torch.tensor(torch.from_numpy(x_temp).float())\n",
    "#         x_temp = x_temp.to(args.device)\n",
    "#         with torch.no_grad():\n",
    "#             p_out, output = net_glob(x_temp)\n",
    "#             pseudo = torch.softmax(p_out.detach_(), dim=-1)\n",
    "#             max_probs, targets_u = torch.max(pseudo, dim=-1)\n",
    "#             if max_probs > tao:\n",
    "#                 x_pusedo.append(x_temp.cpu().numpy())\n",
    "#                 y_pusedo.append(int(targets_u.cpu().numpy()))\n",
    "#                 y_true.append(label[i])\n",
    "#     x_pusedo = np.array(x_pusedo)\n",
    "#     x_pusedo = np.squeeze(x_pusedo)\n",
    "#     y_pusedo = np.array(y_pusedo)\n",
    "#     y_true = np.array(y_true)\n",
    "#     x_pusedo = torch.from_numpy(x_pusedo).float()\n",
    "#     y_pusedo = torch.from_numpy(y_pusedo)\n",
    "#     y_true = torch.from_numpy(y_true)\n",
    "#     return (x_pusedo, y_pusedo,y_true)\n",
    "# def local_update(args, net, data, target,tao=0.99):\n",
    "\n",
    "#     traindata =TensorDataset(data.clone().detach(),target.clone().detach().long())\n",
    "#     ldr_train = DataLoader(traindata, batch_size = args.local_bs, shuffle=True)\n",
    "    \n",
    "#     net.train()\n",
    "#     # train and update\n",
    "#     optimizer = torch.optim.Adam(net.parameters(), lr=args.lr)\n",
    "#     epoch_loss = []\n",
    "#     for iter in range(args.local_ep):\n",
    "#         batch_loss = []\n",
    "#         for batch_idx, (images, labels) in enumerate(ldr_train):\n",
    "#             images, labels = images.to(args.device), labels.to(args.device)\n",
    "#             net.zero_grad()\n",
    "#             p_out,log_probs = net(images)\n",
    "#             pseudo_label = torch.softmax(p_out.detach_(), dim=-1)\n",
    "#             max_probs, targets_u = torch.max(pseudo_label, dim=-1)\n",
    "#             mask = max_probs.ge(tao).float()\n",
    "#             labels = targets_u\n",
    "            \n",
    "#             loss =-(F.nll_loss(p_out, labels,reduction='none')*mask).mean()\n",
    "            \n",
    "#             loss.requires_grad=True\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             if args.verbose and batch_idx % 10 == 0:\n",
    "#                 print('Update Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                     iter, batch_idx * len(images), len(ldr_train.dataset),\n",
    "#                            100. * batch_idx / len(ldr_train), loss.item()))\n",
    "#             batch_loss.append(loss.item())\n",
    "#         epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "#     return net.state_dict(), sum(epoch_loss) / len(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0,
     24,
     37,
     58
    ]
   },
   "outputs": [],
   "source": [
    "def global_update(args, net, data, target):  \n",
    "    traindata =TensorDataset(data.clone().detach(),target.clone().detach().long())\n",
    "    ldr_train = DataLoader(traindata, batch_size = 256, shuffle=True)\n",
    "    \n",
    "    net.train()\n",
    "    # train and update\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=args.lr)\n",
    "    epoch_loss = []\n",
    "    for iter in range(30):\n",
    "        batch_loss = []\n",
    "        for batch_idx, (images, labels) in enumerate(ldr_train):\n",
    "            images, labels = images.to(args.device), labels.to(args.device)\n",
    "            net.zero_grad()\n",
    "            p_out,output = net(images)\n",
    "            loss =F.nll_loss(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if args.verbose and batch_idx % 10 == 0:\n",
    "                print('Update Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    iter, batch_idx * len(images), len(ldr_train.dataset),\n",
    "                           100. * batch_idx / len(ldr_train), loss.item()))\n",
    "            batch_loss.append(loss.item())\n",
    "        epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "    return net.state_dict(), sum(epoch_loss) / len(epoch_loss)\n",
    "def random_group_users(num_users, num_groups):\n",
    "    groups = []\n",
    "    g = int(num_users / num_groups)\n",
    "    users = [i for i in range(num_users)]\n",
    "    for i in range(num_groups):\n",
    "        if i == num_groups - 1:\n",
    "            groups.append(np.array(users))\n",
    "            break\n",
    "        idxs = np.random.choice(users, g)\n",
    "        users = list(set(users) - set(idxs))\n",
    "        groups.append(idxs)\n",
    "    return groups\n",
    "\n",
    "def calc_non_iid(args,net_g=None,dataset=None,idxs=None):   \n",
    "    net_g.eval()\n",
    "    idxs=list(idxs)\n",
    "    image = dataset_unlabel[0][idxs]\n",
    "    label = dataset_unlabel[1][idxs]\n",
    "    data =TensorDataset(image.clone().detach(), label.clone().detach())\n",
    "    data_loader = DataLoader(data, batch_size = args.bs)\n",
    "    re_y=[]\n",
    "    for idx, (data, target) in enumerate(data_loader):\n",
    "        if args.gpu != -1:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        p_out,log_probs = net_g(data)\n",
    "        y_pred = log_probs.data.max(1, keepdim=True)[1]\n",
    "        re_y.extend(y_pred.cpu().numpy().squeeze())\n",
    "    r_count=[]\n",
    "    pro_label=[0,0,0,0,0]\n",
    "    r_count=Counter(re_y)\n",
    "    for i in range(len(pro_label)):\n",
    "        pro_label[i]=r_count[i]/len(image)\n",
    "    return pro_label\n",
    "\n",
    "def group_users(idxs_users, num_groups, cluster_labels):\n",
    "    groups = []\n",
    "    for i in range(num_groups):\n",
    "        groups.append([])\n",
    "    for i in range(len(idxs_users)):\n",
    "        groups[cluster_labels[i]].append(idxs_users[i])\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = []\n",
    "user_target = []\n",
    "for i in range(args.num_users):\n",
    "    image = dataset_unlabel[0][dict_users[i]]\n",
    "    label = dataset_unlabel[1][dict_users[i]]\n",
    "    user_data.append(image)\n",
    "    user_target.append(label)\n",
    "w_glob = net_glob.state_dict()\n",
    "w_save = copy.deepcopy(w_glob)\n",
    "# training\n",
    "loss_train = []\n",
    "cv_loss, cv_acc = [], []\n",
    "val_loss_pre, counter = 0, 0\n",
    "net_best = None\n",
    "best_loss = None\n",
    "val_acc_list, net_list = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-412ab2a1e91c>:16: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
      "  cluster = KMeans(n_clusters=num_groups,random_state=0).fit(client_label_distribution)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample user num:10, Groups: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 170.28 sec:\n",
      "Training accuracy: 23.50\n",
      "Testing accuracy: 24.14\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 171.35 sec:\n",
      "Training accuracy: 78.20\n",
      "Testing accuracy: 79.52\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 169.10 sec:\n",
      "Training accuracy: 79.75\n",
      "Testing accuracy: 80.09\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 166.94 sec:\n",
      "Training accuracy: 87.53\n",
      "Testing accuracy: 87.62\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 167.79 sec:\n",
      "Training accuracy: 86.40\n",
      "Testing accuracy: 87.58\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 168.53 sec:\n",
      "Training accuracy: 85.80\n",
      "Testing accuracy: 86.64\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 169.41 sec:\n",
      "Training accuracy: 86.70\n",
      "Testing accuracy: 86.03\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 170.59 sec:\n",
      "Training accuracy: 85.07\n",
      "Testing accuracy: 86.03\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 169.99 sec:\n",
      "Training accuracy: 86.78\n",
      "Testing accuracy: 87.30\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 184.25 sec:\n",
      "Training accuracy: 87.55\n",
      "Testing accuracy: 88.40\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 213.46 sec:\n",
      "Training accuracy: 88.55\n",
      "Testing accuracy: 89.17\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 264.69 sec:\n",
      "Training accuracy: 88.60\n",
      "Testing accuracy: 88.93\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 310.80 sec:\n",
      "Training accuracy: 87.38\n",
      "Testing accuracy: 87.66\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 308.24 sec:\n",
      "Training accuracy: 87.75\n",
      "Testing accuracy: 88.23\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 311.07 sec:\n",
      "Training accuracy: 89.00\n",
      "Testing accuracy: 88.97\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 285.05 sec:\n",
      "Training accuracy: 89.93\n",
      "Testing accuracy: 90.07\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 285.80 sec:\n",
      "Training accuracy: 89.65\n",
      "Testing accuracy: 90.07\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 283.24 sec:\n",
      "Training accuracy: 90.25\n",
      "Testing accuracy: 89.70\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 285.04 sec:\n",
      "Training accuracy: 89.78\n",
      "Testing accuracy: 89.41\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 282.54 sec:\n",
      "Training accuracy: 90.12\n",
      "Testing accuracy: 90.15\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 213.59 sec:\n",
      "Training accuracy: 89.95\n",
      "Testing accuracy: 89.54\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 213.87 sec:\n",
      "Training accuracy: 90.25\n",
      "Testing accuracy: 89.94\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 212.41 sec:\n",
      "Training accuracy: 88.95\n",
      "Testing accuracy: 89.01\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 213.99 sec:\n",
      "Training accuracy: 89.70\n",
      "Testing accuracy: 89.54\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 211.64 sec:\n",
      "Training accuracy: 89.50\n",
      "Testing accuracy: 88.88\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 211.44 sec:\n",
      "Training accuracy: 89.72\n",
      "Testing accuracy: 89.94\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 212.92 sec:\n",
      "Training accuracy: 90.55\n",
      "Testing accuracy: 90.27\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 212.88 sec:\n",
      "Training accuracy: 90.32\n",
      "Testing accuracy: 89.82\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 213.30 sec:\n",
      "Training accuracy: 90.30\n",
      "Testing accuracy: 90.15\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 213.09 sec:\n",
      "Training accuracy: 89.93\n",
      "Testing accuracy: 89.74\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 213.15 sec:\n",
      "Training accuracy: 90.45\n",
      "Testing accuracy: 89.98\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 212.77 sec:\n",
      "Training accuracy: 89.85\n",
      "Testing accuracy: 89.82\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 214.37 sec:\n",
      "Training accuracy: 89.82\n",
      "Testing accuracy: 89.58\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 214.97 sec:\n",
      "Training accuracy: 90.57\n",
      "Testing accuracy: 89.74\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 212.65 sec:\n",
      "Training accuracy: 90.00\n",
      "Testing accuracy: 89.41\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 215.19 sec:\n",
      "Training accuracy: 90.38\n",
      "Testing accuracy: 89.70\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 211.46 sec:\n",
      "Training accuracy: 89.72\n",
      "Testing accuracy: 89.54\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 213.53 sec:\n",
      "Training accuracy: 90.40\n",
      "Testing accuracy: 89.41\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 214.88 sec:\n",
      "Training accuracy: 90.25\n",
      "Testing accuracy: 89.90\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 211.87 sec:\n",
      "Training accuracy: 90.03\n",
      "Testing accuracy: 89.09\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 213.30 sec:\n",
      "Training accuracy: 90.00\n",
      "Testing accuracy: 89.74\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 210.53 sec:\n",
      "Training accuracy: 90.10\n",
      "Testing accuracy: 89.94\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 212.04 sec:\n",
      "Training accuracy: 90.30\n",
      "Testing accuracy: 90.39\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 212.47 sec:\n",
      "Training accuracy: 89.97\n",
      "Testing accuracy: 89.74\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 213.07 sec:\n",
      "Training accuracy: 89.82\n",
      "Testing accuracy: 89.41\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 211.61 sec:\n",
      "Training accuracy: 90.40\n",
      "Testing accuracy: 89.94\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 212.76 sec:\n",
      "Training accuracy: 90.15\n",
      "Testing accuracy: 90.23\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 212.39 sec:\n",
      "Training accuracy: 89.90\n",
      "Testing accuracy: 89.58\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 209.91 sec:\n",
      "Training accuracy: 90.38\n",
      "Testing accuracy: 89.90\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 214.00 sec:\n",
      "Training accuracy: 90.12\n",
      "Testing accuracy: 90.31\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 209.86 sec:\n",
      "Training accuracy: 89.80\n",
      "Testing accuracy: 89.94\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 212.18 sec:\n",
      "Training accuracy: 90.07\n",
      "Testing accuracy: 89.41\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 212.33 sec:\n",
      "Training accuracy: 90.30\n",
      "Testing accuracy: 90.07\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 210.60 sec:\n",
      "Training accuracy: 90.07\n",
      "Testing accuracy: 90.02\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 160.67 sec:\n",
      "Training accuracy: 90.18\n",
      "Testing accuracy: 89.58\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 162.19 sec:\n",
      "Training accuracy: 90.18\n",
      "Testing accuracy: 89.45\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 160.46 sec:\n",
      "Training accuracy: 90.75\n",
      "Testing accuracy: 90.43\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 164.90 sec:\n",
      "Training accuracy: 90.12\n",
      "Testing accuracy: 89.66\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 160.56 sec:\n",
      "Training accuracy: 89.93\n",
      "Testing accuracy: 89.94\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 167.66 sec:\n",
      "Training accuracy: 90.30\n",
      "Testing accuracy: 90.15\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 168.44 sec:\n",
      "Training accuracy: 90.35\n",
      "Testing accuracy: 90.19\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 166.02 sec:\n",
      "Training accuracy: 90.72\n",
      "Testing accuracy: 90.19\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 163.67 sec:\n",
      "Training accuracy: 90.20\n",
      "Testing accuracy: 89.86\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 165.41 sec:\n",
      "Training accuracy: 90.40\n",
      "Testing accuracy: 89.78\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 168.40 sec:\n",
      "Training accuracy: 90.07\n",
      "Testing accuracy: 90.15\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 170.11 sec:\n",
      "Training accuracy: 90.10\n",
      "Testing accuracy: 90.07\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 166.45 sec:\n",
      "Training accuracy: 90.03\n",
      "Testing accuracy: 90.11\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 169.69 sec:\n",
      "Training accuracy: 90.18\n",
      "Testing accuracy: 89.98\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 168.02 sec:\n",
      "Training accuracy: 90.32\n",
      "Testing accuracy: 90.02\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 172.43 sec:\n",
      "Training accuracy: 90.47\n",
      "Testing accuracy: 90.39\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 166.43 sec:\n",
      "Training accuracy: 90.75\n",
      "Testing accuracy: 90.19\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 163.59 sec:\n",
      "Training accuracy: 90.60\n",
      "Testing accuracy: 90.19\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 163.38 sec:\n",
      "Training accuracy: 90.10\n",
      "Testing accuracy: 89.82\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 165.14 sec:\n",
      "Training accuracy: 90.47\n",
      "Testing accuracy: 89.70\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 171.29 sec:\n",
      "Training accuracy: 90.85\n",
      "Testing accuracy: 90.43\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 159.24 sec:\n",
      "Training accuracy: 90.88\n",
      "Testing accuracy: 90.39\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 160.46 sec:\n",
      "Training accuracy: 90.45\n",
      "Testing accuracy: 90.31\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 159.69 sec:\n",
      "Training accuracy: 90.12\n",
      "Testing accuracy: 89.54\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 159.81 sec:\n",
      "Training accuracy: 90.10\n",
      "Testing accuracy: 89.82\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 158.79 sec:\n",
      "Training accuracy: 90.47\n",
      "Testing accuracy: 90.35\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 159.76 sec:\n",
      "Training accuracy: 90.18\n",
      "Testing accuracy: 90.15\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 158.20 sec:\n",
      "Training accuracy: 91.30\n",
      "Testing accuracy: 90.76\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 157.87 sec:\n",
      "Training accuracy: 90.38\n",
      "Testing accuracy: 90.27\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 157.76 sec:\n",
      "Training accuracy: 90.78\n",
      "Testing accuracy: 90.07\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 157.61 sec:\n",
      "Training accuracy: 90.80\n",
      "Testing accuracy: 90.23\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 157.24 sec:\n",
      "Training accuracy: 91.05\n",
      "Testing accuracy: 90.76\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 156.28 sec:\n",
      "Training accuracy: 90.78\n",
      "Testing accuracy: 90.31\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 157.06 sec:\n",
      "Training accuracy: 90.93\n",
      "Testing accuracy: 90.35\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 157.70 sec:\n",
      "Training accuracy: 91.03\n",
      "Testing accuracy: 90.64\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 158.85 sec:\n",
      "Training accuracy: 91.03\n",
      "Testing accuracy: 89.90\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 158.66 sec:\n",
      "Training accuracy: 90.90\n",
      "Testing accuracy: 90.55\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 158.66 sec:\n",
      "Training accuracy: 91.03\n",
      "Testing accuracy: 90.43\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 155.68 sec:\n",
      "Training accuracy: 91.15\n",
      "Testing accuracy: 90.68\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 158.73 sec:\n",
      "Training accuracy: 90.88\n",
      "Testing accuracy: 90.27\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 157.43 sec:\n",
      "Training accuracy: 91.32\n",
      "Testing accuracy: 90.47\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 158.79 sec:\n",
      "Training accuracy: 90.95\n",
      "Testing accuracy: 90.68\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 158.75 sec:\n",
      "Training accuracy: 90.78\n",
      "Testing accuracy: 89.82\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 158.39 sec:\n",
      "Training accuracy: 91.07\n",
      "Testing accuracy: 90.43\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 157.55 sec:\n",
      "Training accuracy: 91.12\n",
      "Testing accuracy: 90.39\n",
      "sample user num:10, Groups: 3\n",
      "testing dataset length: 4000\n",
      "testing dataset length: 2456\n",
      "time 158.55 sec:\n",
      "Training accuracy: 90.85\n",
      "Testing accuracy: 90.47\n"
     ]
    }
   ],
   "source": [
    "args.frac = 0.5\n",
    "num_groups = 3\n",
    "for iter in range(args.epochs):\n",
    "    start = time.time()\n",
    "    w_locals, loss_locals = [], []\n",
    "    w_gs,loss=global_update(args, net=copy.deepcopy(net_glob).to(args.device),data=dataset_server[0],target=dataset_server[1])\n",
    "\n",
    "    \n",
    "    m = max(int(args.frac * args.num_users), 1)\n",
    "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "    client_label_distribution=[]\n",
    "    for i in idxs_users:\n",
    "        r_l = calc_non_iid(args,net_g=net_glob, dataset=dataset_unlabel, idxs=dict_users[i])\n",
    "        client_label_distribution.append(r_l)\n",
    "    \n",
    "    cluster = KMeans(n_clusters=num_groups,random_state=0).fit(client_label_distribution)\n",
    "    groups  =group_users(idxs_users, num_groups, cluster.labels_)\n",
    "\n",
    "    print(\"sample user num:{:d}, Groups: {:d}\".format(len(idxs_users), num_groups))\n",
    "    for g in groups:\n",
    "        w_group=[]\n",
    "        w_group.append(copy.deepcopy(w_gs))\n",
    "        for idx in g:\n",
    "            w, loss = global_update(args,net=copy.deepcopy(net_glob).to(args.device),data=user_data[idx], target=user_target[idx])\n",
    "            w_group.append(copy.deepcopy(w))\n",
    "            loss_locals.append(copy.deepcopy(loss))\n",
    "        w_gp = FedAvg(w_group)\n",
    "        w_locals.append(copy.deepcopy(w_gp))\n",
    "\n",
    "    w_glob = FedAvg(w_locals)\n",
    "\n",
    "    \n",
    "    net_glob.load_state_dict(w_glob)\n",
    "\n",
    "\n",
    "    net_glob.eval()\n",
    "    acc_train, loss_train = test_img(net_glob, dataset_unlabel, args)\n",
    "    acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "    val_acc_list.append(acc_test)\n",
    "    print(\"time {:.2f} sec:\".format(time.time()-start))\n",
    "    print(\"Training accuracy: {:.2f}\".format(acc_train))\n",
    "    print(\"Testing accuracy: {:.2f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.14495 , 79.51955 , 80.08958 , 87.62215 , 87.581436, 86.64495 ,\n",
       "       86.0342  , 86.0342  , 87.29642 , 88.39577 , 89.16938 , 88.92508 ,\n",
       "       87.662865, 88.2329  , 88.9658  , 90.06515 , 90.06515 , 89.6987  ,\n",
       "       89.41368 , 90.14658 , 89.53583 , 89.94299 , 89.006516, 89.53583 ,\n",
       "       88.88436 , 89.94299 , 90.26873 , 89.82085 , 90.14658 , 89.73941 ,\n",
       "       89.98371 , 89.82085 , 89.576546, 89.73941 , 89.41368 , 89.6987  ,\n",
       "       89.53583 , 89.41368 , 89.90228 , 89.08795 , 89.73941 , 89.94299 ,\n",
       "       90.39088 , 89.73941 , 89.41368 , 89.94299 , 90.22801 , 89.576546,\n",
       "       89.90228 , 90.30945 , 89.94299 , 89.41368 , 90.06515 , 90.02443 ,\n",
       "       89.576546, 89.4544  , 90.431595, 89.65798 , 89.94299 , 90.14658 ,\n",
       "       90.187294, 90.187294, 89.861565, 89.78013 , 90.14658 , 90.06515 ,\n",
       "       90.105865, 89.98371 , 90.02443 , 90.39088 , 90.187294, 90.187294,\n",
       "       89.82085 , 89.6987  , 90.431595, 90.39088 , 90.30945 , 89.53583 ,\n",
       "       89.82085 , 90.35017 , 90.14658 , 90.75733 , 90.26873 , 90.06515 ,\n",
       "       90.22801 , 90.75733 , 90.30945 , 90.35017 , 90.63518 , 89.90228 ,\n",
       "       90.55375 , 90.431595, 90.675896, 90.26873 , 90.47231 , 90.675896,\n",
       "       89.82085 , 90.431595, 90.39088 , 90.47231 ], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc=[]\n",
    "for x in val_acc_list:\n",
    "    acc.append(x.numpy())\n",
    "acc=np.array(acc)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30.456026, 58.346905, 77.72801 , 83.754074, 81.02606 , 87.62215 ,\n",
       "       86.807816, 86.97068 , 86.92997 , 88.02932 , 88.80293 , 87.78502 ,\n",
       "       89.4544  , 87.7443  , 88.35505 , 89.332245, 89.576546, 88.55863 ,\n",
       "       89.94299 , 88.59935 , 89.90228 , 89.861565, 89.82085 , 89.65798 ,\n",
       "       90.06515 , 88.517914, 88.31433 , 90.02443 , 90.14658 , 89.78013 ,\n",
       "       88.68078 , 88.80293 , 89.82085 , 90.35017 , 88.80293 , 89.861565,\n",
       "       89.4544  , 89.94299 , 89.25082 , 89.94299 , 89.94299 , 88.84365 ,\n",
       "       89.006516, 89.16938 , 90.55375 , 90.14658 , 89.78013 , 89.08795 ,\n",
       "       89.006516, 89.94299 , 89.73941 , 89.61726 , 89.65798 , 90.14658 ,\n",
       "       89.37296 , 89.94299 , 89.16938 , 89.73941 , 88.59935 , 89.82085 ,\n",
       "       88.59935 , 89.78013 , 88.477196, 89.53583 , 88.88436 , 89.73941 ,\n",
       "       88.64007 , 89.41368 , 89.6987  , 89.2101  , 89.861565, 89.61726 ,\n",
       "       89.291534, 89.78013 , 89.25082 , 89.49512 , 89.78013 , 88.64007 ,\n",
       "       88.59935 , 89.82085 , 89.37296 , 89.12866 , 88.2329  , 88.7215  ,\n",
       "       89.61726 , 87.78502 , 89.861565, 89.576546, 89.576546, 89.25082 ,\n",
       "       88.2329  , 89.332245, 89.4544  , 89.37296 , 89.73941 , 89.61726 ,\n",
       "       88.762215, 89.78013 , 89.98371 , 89.576546], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc=[]\n",
    "for x in val_acc_list:\n",
    "    acc.append(x.numpy())\n",
    "acc=np.array(acc)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f2e958c46c8c>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  traindata =TensorDataset(torch.tensor(data),torch.tensor(target,dtype=torch.long))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample user num: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-8fed76bb45ea>:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  traindata =TensorDataset(torch.tensor(data),torch.tensor(target,dtype=torch.long))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round   0, Average loss 0.073\n",
      "\n",
      "testing dataset length: 12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuanshao/Semi_FL/utils.py:233: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data =TensorDataset(torch.tensor(image),torch.tensor(label,dtype=torch.long))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing dataset length: 2456\n",
      "time 30.51 sec:\n",
      "Training accuracy: 31.39\n",
      "Testing accuracy: 30.25\n",
      "sample user num: 10\n",
      "Round   1, Average loss 0.062\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.56 sec:\n",
      "Training accuracy: 31.03\n",
      "Testing accuracy: 29.68\n",
      "sample user num: 10\n",
      "Round   2, Average loss 0.056\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.45 sec:\n",
      "Training accuracy: 31.63\n",
      "Testing accuracy: 30.01\n",
      "sample user num: 10\n",
      "Round   3, Average loss 0.054\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.15 sec:\n",
      "Training accuracy: 31.27\n",
      "Testing accuracy: 30.01\n",
      "sample user num: 10\n",
      "Round   4, Average loss 0.049\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.00 sec:\n",
      "Training accuracy: 30.98\n",
      "Testing accuracy: 29.72\n",
      "sample user num: 10\n",
      "Round   5, Average loss 0.047\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.52 sec:\n",
      "Training accuracy: 31.09\n",
      "Testing accuracy: 29.85\n",
      "sample user num: 10\n",
      "Round   6, Average loss 0.043\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.52 sec:\n",
      "Training accuracy: 34.69\n",
      "Testing accuracy: 33.14\n",
      "sample user num: 10\n",
      "Round   7, Average loss 0.041\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.53 sec:\n",
      "Training accuracy: 38.58\n",
      "Testing accuracy: 36.89\n",
      "sample user num: 10\n",
      "Round   8, Average loss 0.039\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.56 sec:\n",
      "Training accuracy: 43.77\n",
      "Testing accuracy: 42.26\n",
      "sample user num: 10\n",
      "Round   9, Average loss 0.037\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.56 sec:\n",
      "Training accuracy: 49.72\n",
      "Testing accuracy: 48.82\n",
      "sample user num: 10\n",
      "Round  10, Average loss 0.035\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.66 sec:\n",
      "Training accuracy: 59.31\n",
      "Testing accuracy: 58.92\n",
      "sample user num: 10\n",
      "Round  11, Average loss 0.034\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.54 sec:\n",
      "Training accuracy: 62.23\n",
      "Testing accuracy: 62.09\n",
      "sample user num: 10\n",
      "Round  12, Average loss 0.032\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.53 sec:\n",
      "Training accuracy: 65.56\n",
      "Testing accuracy: 65.88\n",
      "sample user num: 10\n",
      "Round  13, Average loss 0.032\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.52 sec:\n",
      "Training accuracy: 70.07\n",
      "Testing accuracy: 70.89\n",
      "sample user num: 10\n",
      "Round  14, Average loss 0.030\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.56 sec:\n",
      "Training accuracy: 71.68\n",
      "Testing accuracy: 72.27\n",
      "sample user num: 10\n",
      "Round  15, Average loss 0.029\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.54 sec:\n",
      "Training accuracy: 73.47\n",
      "Testing accuracy: 73.70\n",
      "sample user num: 10\n",
      "Round  16, Average loss 0.028\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.63 sec:\n",
      "Training accuracy: 75.43\n",
      "Testing accuracy: 75.00\n",
      "sample user num: 10\n",
      "Round  17, Average loss 0.028\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.48 sec:\n",
      "Training accuracy: 74.97\n",
      "Testing accuracy: 74.76\n",
      "sample user num: 10\n",
      "Round  18, Average loss 0.028\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 28.31 sec:\n",
      "Training accuracy: 75.38\n",
      "Testing accuracy: 76.06\n",
      "sample user num: 10\n",
      "Round  19, Average loss 0.033\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 28.61 sec:\n",
      "Training accuracy: 75.78\n",
      "Testing accuracy: 75.94\n",
      "sample user num: 10\n",
      "Round  20, Average loss 0.048\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.68 sec:\n",
      "Training accuracy: 76.55\n",
      "Testing accuracy: 77.16\n",
      "sample user num: 10\n",
      "Round  21, Average loss 0.075\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.60 sec:\n",
      "Training accuracy: 78.24\n",
      "Testing accuracy: 78.87\n",
      "sample user num: 10\n",
      "Round  22, Average loss 0.151\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.62 sec:\n",
      "Training accuracy: 78.61\n",
      "Testing accuracy: 78.99\n",
      "sample user num: 10\n",
      "Round  23, Average loss 0.210\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.58 sec:\n",
      "Training accuracy: 79.25\n",
      "Testing accuracy: 79.68\n",
      "sample user num: 10\n",
      "Round  24, Average loss 0.297\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.53 sec:\n",
      "Training accuracy: 77.93\n",
      "Testing accuracy: 78.42\n",
      "sample user num: 10\n",
      "Round  25, Average loss 0.396\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.68 sec:\n",
      "Training accuracy: 76.84\n",
      "Testing accuracy: 77.69\n",
      "sample user num: 10\n",
      "Round  26, Average loss 0.511\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.60 sec:\n",
      "Training accuracy: 79.18\n",
      "Testing accuracy: 79.48\n",
      "sample user num: 10\n",
      "Round  27, Average loss 0.646\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.65 sec:\n",
      "Training accuracy: 79.90\n",
      "Testing accuracy: 80.29\n",
      "sample user num: 10\n",
      "Round  28, Average loss 0.805\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.66 sec:\n",
      "Training accuracy: 80.23\n",
      "Testing accuracy: 80.01\n",
      "sample user num: 10\n",
      "Round  29, Average loss 1.060\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.84 sec:\n",
      "Training accuracy: 80.55\n",
      "Testing accuracy: 80.62\n",
      "sample user num: 10\n",
      "Round  30, Average loss 1.042\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.56 sec:\n",
      "Training accuracy: 79.81\n",
      "Testing accuracy: 80.09\n",
      "sample user num: 10\n",
      "Round  31, Average loss 1.193\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 29.72 sec:\n",
      "Training accuracy: 80.44\n",
      "Testing accuracy: 80.99\n",
      "sample user num: 10\n",
      "Round  32, Average loss 1.406\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.59 sec:\n",
      "Training accuracy: 81.06\n",
      "Testing accuracy: 81.07\n",
      "sample user num: 10\n",
      "Round  33, Average loss 1.290\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.28 sec:\n",
      "Training accuracy: 80.20\n",
      "Testing accuracy: 80.42\n",
      "sample user num: 10\n",
      "Round  34, Average loss 1.457\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.53 sec:\n",
      "Training accuracy: 80.50\n",
      "Testing accuracy: 80.46\n",
      "sample user num: 10\n",
      "Round  35, Average loss 1.533\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.64 sec:\n",
      "Training accuracy: 80.18\n",
      "Testing accuracy: 80.58\n",
      "sample user num: 10\n",
      "Round  36, Average loss 1.825\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.55 sec:\n",
      "Training accuracy: 80.57\n",
      "Testing accuracy: 81.07\n",
      "sample user num: 10\n",
      "Round  37, Average loss 2.009\n",
      "\n",
      "testing dataset length: 12000\n",
      "testing dataset length: 2456\n",
      "time 30.57 sec:\n",
      "Training accuracy: 81.05\n",
      "Testing accuracy: 81.19\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-37b884b3e794>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mw_locals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_locals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobe_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_glob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_server\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_server\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mw_locals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mloss_locals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-f2e958c46c8c>\u001b[0m in \u001b[0;36mglobe_update\u001b[0;34m(args, net, data, target)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mp_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args.frac=0.5\n",
    "for iter in range(args.epochs):\n",
    "    start=time.time()\n",
    "    w_locals, loss_locals = [], []\n",
    "    w,loss=globe_update(args ,net=copy.deepcopy(net_glob).to(args.device),data=dataset_server[0],target=dataset_server[1])\n",
    "    w_locals.append(copy.deepcopy(w))\n",
    "    loss_locals.append(copy.deepcopy(loss))\n",
    "    \n",
    "    w_locals.append(copy.deepcopy(w_glob))\n",
    "    m = max(int(args.frac * args.num_users), 1)\n",
    "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "    print(\"sample user num:\",len(idxs_users))\n",
    "    for idx in idxs_users:\n",
    "        w, loss = local_update(args,net=copy.deepcopy(net_glob).to(args.device),data=user_data[idx],target=user_target[idx])\n",
    "\n",
    "        #         w, loss = local_update_label(args,net=copy.deepcopy(net_glob).to(args.device),data=user_data[idx],target=user_target[idx])\n",
    "        w_locals.append(copy.deepcopy(w))\n",
    "        loss_locals.append(copy.deepcopy(loss))\n",
    "    # update global weights\n",
    "\n",
    "    w_glob = FedAvg(w_locals)\n",
    "\n",
    "    # copy weight to net_glob\n",
    "    net_glob.load_state_dict(w_glob)\n",
    "\n",
    "    # print loss\n",
    "    loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "    print('Round {:3d}, Average loss {:.3f}'.format(iter, loss_avg))\n",
    "#     loss_train.append(loss_avg)\n",
    "\n",
    "    # testing\n",
    "    net_glob.eval()\n",
    "    print()\n",
    "    acc_train, loss_train = test_img(net_glob, dataset_unlabel, args)\n",
    "    acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "    val_acc_list.append(acc_test)\n",
    "    print(\"time {:.2f} sec:\".format(time.time()-start))\n",
    "    print(\"Training accuracy: {:.2f}\".format(acc_train))\n",
    "    print(\"Testing accuracy: {:.2f}\".format(acc_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "387px",
    "left": "102px",
    "right": "20px",
    "top": "50px",
    "width": "781px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
